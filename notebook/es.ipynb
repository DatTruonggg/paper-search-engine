{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f235ab",
   "metadata": {},
   "source": [
    "# Elasticsearch Paper ID Explorer\n",
    "\n",
    "Notebook này giúp liệt kê tất cả `paper_id` đang có trong index Elasticsearch và thống kê số lượng chunk.\n",
    "\n",
    "\n",
    "## Nội dung\n",
    "1. Cấu hình kết nối\n",
    "2. Lấy danh sách paper_id (terms aggregation)\n",
    "3. Lấy danh sách paper_id (scroll fallback)\n",
    "4. Thống kê papers vs chunks\n",
    "5. Xuất CSV\n",
    "\n",
    "Chỉnh `ES_HOST` nếu cần (ví dụ `http://103.3.247.120:9202`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8966845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ES_HOST=http://103.3.247.120:9202, INDEX=papers\n",
      "Cluster name: paper-search-cluster\n"
     ]
    }
   ],
   "source": [
    "# 1. Cấu hình kết nối\n",
    "import os, json, math\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "\n",
    "ES_HOST = os.environ.get(\"ES_HOST\", \"http://103.3.247.120:9202\")\n",
    "INDEX = os.environ.get(\"ES_INDEX_NAME\", \"papers\")\n",
    "SESSION = requests.Session()\n",
    "print(f\"Using ES_HOST={ES_HOST}, INDEX={INDEX}\")\n",
    "\n",
    "# Helper\n",
    "def es_get(path: str, body: Dict=None):\n",
    "    url = f\"{ES_HOST}/{path}\" if not path.startswith('http') else path\n",
    "    if body is None:\n",
    "        r = SESSION.get(url, timeout=30)\n",
    "    else:\n",
    "        r = SESSION.post(url, json=body, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# Kiểm tra cluster\n",
    "try:\n",
    "    info = es_get(\"\")\n",
    "    print(\"Cluster name:\", info.get(\"cluster_name\"))\n",
    "except Exception as e:\n",
    "    print(\"Không kết nối được Elasticsearch:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "560fefc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agg mode: lấy được 20 paper_id (top 20)\n"
     ]
    }
   ],
   "source": [
    "# 2. Lấy danh sách paper_id bằng terms aggregation (nhanh, giới hạn top N)\n",
    "TOP_N = 20  # đổi nếu cần\n",
    "agg_body = {\n",
    "    \"size\": 0,\n",
    "    \"query\": {\"term\": {\"doc_type\": \"paper\"}},\n",
    "    \"aggs\": {\n",
    "        \"papers\": {\n",
    "            \"terms\": {\"field\": \"paper_id\", \"size\": TOP_N}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "try:\n",
    "    agg_resp = es_get(f\"{INDEX}/_search\", agg_body)\n",
    "    buckets = agg_resp.get('aggregations', {}).get('papers', {}).get('buckets', [])\n",
    "    paper_ids_agg = [b['key'] for b in buckets]\n",
    "    print(f\"Agg mode: lấy được {len(paper_ids_agg)} paper_id (top {TOP_N})\")\n",
    "    paper_ids_agg[:10]\n",
    "except Exception as e:\n",
    "    print(\"Aggregation lỗi:\", e)\n",
    "    paper_ids_agg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afba6376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bỏ qua scroll (USE_SCROLL=False)\n"
     ]
    }
   ],
   "source": [
    "# 3. Scroll fallback để lấy toàn bộ (nếu số lượng lớn hơn terms size)\n",
    "# Chỉ chạy nếu cần đầy đủ và index lớn.\n",
    "USE_SCROLL = False  # chuyển True nếu muốn\n",
    "SCROLL_SIZE = 500\n",
    "all_paper_ids_scroll = []\n",
    "\n",
    "if USE_SCROLL:\n",
    "    first = es_get(f\"{INDEX}/_search?scroll=1m\", {\n",
    "        \"size\": SCROLL_SIZE,\n",
    "        \"_source\": [\"paper_id\"],\n",
    "        \"query\": {\"term\": {\"doc_type\": \"paper\"}}\n",
    "    })\n",
    "    scroll_id = first.get('_scroll_id')\n",
    "    hits = first.get('hits', {}).get('hits', [])\n",
    "    while hits:\n",
    "        for h in hits:\n",
    "            pid = h.get('_source', {}).get('paper_id')\n",
    "            if pid:\n",
    "                all_paper_ids_scroll.append(pid)\n",
    "        nxt = es_get(\"_search/scroll\", {\"scroll\": \"1m\", \"scroll_id\": scroll_id})\n",
    "        scroll_id = nxt.get('_scroll_id')\n",
    "        hits = nxt.get('hits', {}).get('hits', [])\n",
    "    print(f\"Scroll mode: lấy được {len(all_paper_ids_scroll)} paper_id\")\n",
    "else:\n",
    "    print(\"Bỏ qua scroll (USE_SCROLL=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b13c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 papers theo số chunk (value_count trên chunk_index):\n",
      "{'paper_id': '2508.00910', 'chunks': 465, 'docs': 465}\n",
      "{'paper_id': '2210.14275', 'chunks': 425, 'docs': 425}\n",
      "{'paper_id': '2508.21148', 'chunks': 403, 'docs': 403}\n",
      "{'paper_id': '2508.19005', 'chunks': 278, 'docs': 278}\n",
      "{'paper_id': '2503.09454', 'chunks': 249, 'docs': 249}\n",
      "{'paper_id': '2509.02464', 'chunks': 235, 'docs': 235}\n",
      "{'paper_id': '2508.19689', 'chunks': 232, 'docs': 232}\n",
      "{'paper_id': '2509.02547', 'chunks': 229, 'docs': 229}\n",
      "{'paper_id': '2509.02444', 'chunks': 227, 'docs': 227}\n",
      "{'paper_id': '2405.14093', 'chunks': 190, 'docs': 190}\n",
      "{'paper_id': '2508.20109', 'chunks': 183, 'docs': 183}\n",
      "{'paper_id': '2508.07407', 'chunks': 166, 'docs': 166}\n",
      "{'paper_id': '2508.20325', 'chunks': 166, 'docs': 166}\n",
      "{'paper_id': '2509.01324', 'chunks': 166, 'docs': 166}\n",
      "{'paper_id': '2508.20453', 'chunks': 155, 'docs': 155}\n",
      "{'paper_id': '2508.19229', 'chunks': 154, 'docs': 154}\n",
      "{'paper_id': '2509.01909', 'chunks': 154, 'docs': 154}\n",
      "{'paper_id': '2508.19274', 'chunks': 153, 'docs': 153}\n",
      "{'paper_id': '2505.15695', 'chunks': 149, 'docs': 149}\n",
      "{'paper_id': '2508.19294', 'chunks': 149, 'docs': 149}\n"
     ]
    }
   ],
   "source": [
    "# 4. Thống kê số chunk cho mỗi paper (top 20)\n",
    "# Lưu ý: sử dụng terms agg + top_hits để đếm matching chunk docs.\n",
    "chunk_count_body = {\n",
    "    \"size\": 0,\n",
    "    \"query\": {\"term\": {\"doc_type\": \"chunk\"}},\n",
    "    \"aggs\": {\n",
    "        \"papers\": {\n",
    "            \"terms\": {\"field\": \"paper_id\", \"size\": 20},\n",
    "            \"aggs\": {\n",
    "                \"cnt\": {\"value_count\": {\"field\": \"chunk_index\"}}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "try:\n",
    "    cc_resp = es_get(f\"{INDEX}/_search\", chunk_count_body)\n",
    "    cc_buckets = cc_resp.get('aggregations', {}).get('papers', {}).get('buckets', [])\n",
    "    top_chunk_stats = [\n",
    "        {\"paper_id\": b['key'], \"chunks\": b['cnt']['value'], \"docs\": b['doc_count']} for b in cc_buckets\n",
    "    ]\n",
    "    print(\"Top 20 papers theo số chunk (value_count trên chunk_index):\")\n",
    "    for row in top_chunk_stats:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(\"Lỗi thống kê chunk:\", e)\n",
    "    top_chunk_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dedf254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng hợp được 20 paper_id\n",
      "Đã ghi: paper_ids_20250921_071854.csv\n"
     ]
    }
   ],
   "source": [
    "# 5. Xuất danh sách paper_id ra CSV (kết hợp agg + scroll nếu có)\n",
    "import csv, datetime\n",
    "all_ids = set(paper_ids_agg) | set(all_paper_ids_scroll)\n",
    "print(f\"Tổng hợp được {len(all_ids)} paper_id\")\n",
    "output_file = f\"paper_ids_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"paper_id\"])\n",
    "    for pid in sorted(all_ids):\n",
    "        w.writerow([pid])\n",
    "print(\"Đã ghi:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13059cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lấy 20 papers (paper_id, title):\n",
      "('2211.09623', 'Cross-Modal Adapter for Vision-Language Retrieval')\n",
      "('2212.07126', 'Explainability of Text Processing and Retrieval Methods: A Survey')\n",
      "('2301.06375', 'OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset')\n",
      "('2303.08032', 'Verifying the Robustness of Automatic Credibility Assessment')\n",
      "('2405.14314', 'Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration')\n",
      "('2405.14862', 'Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs')\n",
      "('2405.15165', 'SoAy: A Solution-based LLM API-using Methodology for Academic Information Seeking')\n",
      "('2402.14533', 'Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard')\n",
      "('2403.01777', 'NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision')\n",
      "('2403.04931', 'A Survey on Human-AI Collaboration with Large Foundation Models')\n",
      "('2404.01245', 'A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules')\n",
      "('2408.13442', 'A Law of Next-Token Prediction in Large Language Models')\n",
      "('2409.00061', 'Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language')\n",
      "('2409.06679', 'E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning')\n",
      "('2409.07132', 'LLM-based feature generation from text for interpretable machine learning')\n",
      "('2407.20271', 'Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models')\n",
      "('2408.05873', 'Recognizing Limits: Investigating Infeasibility in Large Language Models')\n",
      "('2408.10722', 'MEGen: Generative Backdoor into Large Language Models via Model Editing')\n",
      "('2408.11727', 'Efficient Detection of Toxic Prompts in Large Language Models')\n",
      "('2407.04620', 'Learning to (Learn at Test Time): RNNs with Expressive Hidden States')\n"
     ]
    }
   ],
   "source": [
    "# 6. Liệt kê paper_id cùng paper_title (top N)\n",
    "TOP_N_TITLES = 20  # đổi nếu cần\n",
    "try:\n",
    "    title_body = {\n",
    "        \"size\": TOP_N_TITLES,\n",
    "        \"query\": {\"term\": {\"doc_type\": \"paper\"}},\n",
    "        \"_source\": [\"paper_id\", \"title\", \"paper_title\"],\n",
    "        \"sort\": [\"_doc\"]\n",
    "    }\n",
    "    title_resp = es_get(f\"{INDEX}/_search\", title_body)\n",
    "    hits = title_resp.get('hits', {}).get('hits', [])\n",
    "    rows = []\n",
    "    for h in hits:\n",
    "        src = h.get('_source', {})\n",
    "        pid = src.get('paper_id')\n",
    "        # Một số ingestion có thể dùng 'title' hoặc 'paper_title'\n",
    "        ptitle = src.get('paper_title') or src.get('title') or '<NO_TITLE>'\n",
    "        rows.append((pid, ptitle[:160]))\n",
    "    print(f\"Lấy {len(rows)} papers (paper_id, title):\")\n",
    "    for r in rows:\n",
    "        print(r)\n",
    "except Exception as e:\n",
    "    print(\"Lỗi lấy paper titles:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "czd_papersearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
